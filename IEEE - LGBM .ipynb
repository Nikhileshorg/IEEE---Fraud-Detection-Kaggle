{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from scipy import interp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('E:\\\\Projects folder\\\\ieee-fraud-detection\\\\train_transaction.csv', index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('E:\\\\Projects folder\\\\ieee-fraud-detection\\\\test_transaction.csv', index_col='TransactionID')\n",
    "train_identity = pd.read_csv('E:\\\\Projects folder\\\\ieee-fraud-detection\\\\train_identity.csv', index_col='TransactionID')\n",
    "test_identity = pd.read_csv('E:\\\\Projects folder\\\\ieee-fraud-detection\\\\test_identity.csv', index_col='TransactionID')\n",
    "sample_submission = pd.read_csv('E:\\\\Projects folder\\\\ieee-fraud-detection\\\\sample_submission.csv', index_col='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "train_df = pd.read_pickle('../input/ieee-data-minification/train_transaction.pkl')\n",
    "\n",
    "if LOCAL_TEST:\n",
    "    test_df = train_df.iloc[-100000:,].reset_index(drop=True)\n",
    "    train_df = train_df.iloc[:400000,].reset_index(drop=True)\n",
    "    \n",
    "    train_identity = pd.read_pickle('../input/ieee-data-minification/train_identity.pkl')\n",
    "    test_identity  = train_identity[train_identity['TransactionID'].isin(test_df['TransactionID'])].reset_index(drop=True)\n",
    "    train_identity = train_identity[train_identity['TransactionID'].isin(train_df['TransactionID'])].reset_index(drop=True)\n",
    "else:\n",
    "    test_df = pd.read_pickle('../input/ieee-data-minification/test_transaction.pkl')\n",
    "    train_identity = pd.read_pickle('../input/ieee-data-minification/train_identity.pkl')\n",
    "    test_identity = pd.read_pickle('../input/ieee-data-minification/test_identity.pkl')\n",
    "    \n",
    "base_columns = list(train_df) + list(train_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Columns\n",
    "#################################################################################\n",
    "## Main Data\n",
    "# 'TransactionID',\n",
    "# 'isFraud',\n",
    "# 'TransactionDT',\n",
    "# 'TransactionAmt',\n",
    "# 'ProductCD',\n",
    "# 'card1' - 'card6',\n",
    "# 'addr1' - 'addr2',\n",
    "# 'dist1' - 'dist2',\n",
    "# 'P_emaildomain' - 'R_emaildomain',\n",
    "# 'C1' - 'C14'\n",
    "# 'D1' - 'D15'\n",
    "# 'M1' - 'M9'\n",
    "# 'V1' - 'V339'\n",
    "\n",
    "## Identity Data\n",
    "# 'TransactionID'\n",
    "# 'id_01' - 'id_38'\n",
    "# 'DeviceType',\n",
    "# 'DeviceInfo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### 'P_emaildomain' - 'R_emaildomain'\n",
    "# Lets do small check to see if 'P_emaildomain' - 'R_emaildomain' matters\n",
    "# and if NaN matters\n",
    "if LOCAL_TEST:\n",
    "    print('#'*5, 'Email matchings test')\n",
    "    for df in [train_df, test_df]:\n",
    "        len_match = df[df['P_emaildomain']==df['R_emaildomain']]\n",
    "        len_not_match = df[df['P_emaildomain']!=df['R_emaildomain']]\n",
    "\n",
    "        print('Match', len(len_match[len_match[TARGET]==1])/len(len_match), 'Total items:', len(len_match))\n",
    "        print('Not Match', len(len_not_match[len_not_match[TARGET]==1])/len(len_not_match), 'Total items:', len(len_not_match))\n",
    "        print('#'*5)\n",
    "\n",
    "    print('#'*5, 'Email NaN test')\n",
    "    for df in [train_df, test_df]:\n",
    "        len_match = df[(df['P_emaildomain'].isna())&(df['R_emaildomain'].isna())]\n",
    "        len_not_match = df[['P_emaildomain','R_emaildomain',TARGET]].dropna()\n",
    "\n",
    "        print('All NaN', len(len_match[len_match[TARGET]==1])/len(len_match), 'Total items:', len(len_match))\n",
    "        print('No NaNs', len(len_not_match[len_not_match[TARGET]==1])/len(len_not_match), 'Total items:', len(len_not_match))\n",
    "        print('#'*5)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### 'P_emaildomain' - 'R_emaildomain'\n",
    "# Matching\n",
    "train_df['email_check'] = np.where(train_df['P_emaildomain']==train_df['R_emaildomain'],1,0)\n",
    "test_df['email_check']  = np.where(test_df['P_emaildomain']==test_df['R_emaildomain'],1,0)\n",
    "\n",
    "# All NaNs\n",
    "train_df['email_check_nan_all'] = np.where((train_df['P_emaildomain'].isna())&(train_df['R_emaildomain'].isna()),1,0)\n",
    "test_df['email_check_nan_all']  = np.where((test_df['P_emaildomain'].isna())&(test_df['R_emaildomain'].isna()),1,0)\n",
    "\n",
    "# Any NaN\n",
    "train_df['email_check_nan_any'] = np.where((train_df['P_emaildomain'].isna())|(train_df['R_emaildomain'].isna()),1,0)\n",
    "test_df['email_check_nan_any']  = np.where((test_df['P_emaildomain'].isna())|(test_df['R_emaildomain'].isna()),1,0)\n",
    "\n",
    "# Fix NaN, get \"prefix\"\n",
    "def fix_emails(df):\n",
    "    df['P_emaildomain'] = df['P_emaildomain'].fillna('email_not_provided')\n",
    "    df['R_emaildomain'] = df['R_emaildomain'].fillna('email_not_provided')\n",
    "    \n",
    "    df['email_match_not_nan'] = np.where((df['P_emaildomain']==df['R_emaildomain'])&\n",
    "                                     (df['P_emaildomain']!='email_not_provided'),1,0)\n",
    "    \n",
    "    df['P_email_prefix'] = df['P_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "    df['R_email_prefix'] = df['R_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "    return df\n",
    "\n",
    "train_df = fix_emails(train_df)\n",
    "test_df = fix_emails(test_df)\n",
    "\n",
    "## Local test doesn't show any boost here, \n",
    "## but I think it's good option for model stability \n",
    "\n",
    "## Also, we will do frequency enconing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### D9 and TransactionDT\n",
    "# Seems that D9 column is a hour\n",
    "# But what hour?\n",
    "# Local time? Server time? Shop time?\n",
    "# Previous transaction? Most common time for client?\n",
    "# Is there difference between TransactionDT and D9 column?\n",
    "# Is there connection with distance?\n",
    "train_df['local_hour'] = train_df['D9']*24\n",
    "test_df['local_hour']  = test_df['D9']*24\n",
    "\n",
    "train_df['local_hour'] = train_df['local_hour'] - (train_df['TransactionDT']/(60*60))%24\n",
    "test_df['local_hour']  = test_df['local_hour'] - (test_df['TransactionDT']/(60*60))%24\n",
    "\n",
    "train_df['local_hour_dist'] = train_df['local_hour']/train_df['dist2']\n",
    "test_df['local_hour_dist']  = test_df['local_hour']/test_df['dist2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### M columns (except M4)\n",
    "# All these columns are binary encoded 1/0\n",
    "# We can have some features from it\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "train_df['M_sum'] = train_df[i_cols].sum(axis=1).astype(np.int8)\n",
    "test_df['M_sum']  = test_df[i_cols].sum(axis=1).astype(np.int8)\n",
    "\n",
    "train_df['M_na'] = train_df[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "test_df['M_na']  = test_df[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "train_df['M_type'] = ''\n",
    "test_df['M_type']  = ''\n",
    "\n",
    "for col in i_cols:\n",
    "    train_df['M_type'] = '_'+train_df[col].astype(str)\n",
    "    test_df['M_type'] = '_'+test_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### C columns\n",
    "# C columns are some counts, based on client identity\n",
    "# Most popular Value is \"1\" -> that seems to be just a single match \n",
    "# (New or stable client)\n",
    "# You can check that auc score for that cliens are lower than global\n",
    "# Lets encode such client types\n",
    "\n",
    "i_cols = ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']\n",
    "\n",
    "train_df['C_sum'] = 0\n",
    "test_df['C_sum']  = 0\n",
    "\n",
    "train_df['C_null'] = 0\n",
    "test_df['C_null']  = 0\n",
    "\n",
    "for col in i_cols:\n",
    "    train_df['C_sum'] += np.where(train_df[col]==1,1,0)\n",
    "    test_df['C_sum']  += np.where(test_df[col]==1,1,0)\n",
    "\n",
    "    train_df['C_null'] += np.where(train_df[col]==0,1,0)\n",
    "    test_df['C_null']  += np.where(test_df[col]==0,1,0)\n",
    "    \n",
    "    valid_values = train_df[col].value_counts()\n",
    "    valid_values = valid_values[valid_values>1000]\n",
    "    valid_values = list(valid_values.index)\n",
    "    \n",
    "    train_df[col+'_valid'] = np.where(train_df[col].isin(valid_values),1,0)\n",
    "    test_df[col+'_valid']  = np.where(test_df[col].isin(valid_values),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Reset values for \"noise\" card1\n",
    "valid_card = train_df['card1'].value_counts()\n",
    "valid_card = valid_card[valid_card>10]\n",
    "valid_card = list(valid_card.index)\n",
    "    \n",
    "train_df['card1'] = np.where(train_df['card1'].isin(valid_card), train_df['card1'], np.nan)\n",
    "test_df['card1']  = np.where(test_df['card1'].isin(valid_card), test_df['card1'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Device info\n",
    "train_identity['DeviceInfo'] = train_identity['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "test_identity['DeviceInfo'] = test_identity['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "\n",
    "train_identity['DeviceInfo_c'] = train_identity['DeviceInfo']\n",
    "test_identity['DeviceInfo_c'] = test_identity['DeviceInfo']\n",
    "\n",
    "device_match_dict = {\n",
    "    'sm':'sm-',\n",
    "    'sm':'samsung',\n",
    "    'huawei':'huawei',\n",
    "    'moto':'moto',\n",
    "    'rv':'rv:',\n",
    "    'trident':'trident',\n",
    "    'lg':'lg-',\n",
    "    'htc':'htc',\n",
    "    'blade':'blade',\n",
    "    'windows':'windows',\n",
    "    'lenovo':'lenovo',\n",
    "    'linux':'linux',\n",
    "    'f3':'f3',\n",
    "    'f5':'f5'\n",
    "}\n",
    "for dev_type_s, dev_type_o in device_match_dict.items():\n",
    "    train_identity['DeviceInfo_c'] = train_identity['DeviceInfo_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    test_identity['DeviceInfo_c'] = test_identity['DeviceInfo_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "\n",
    "train_identity['DeviceInfo_c'] = train_identity['DeviceInfo_c'].apply(lambda x: 'other_d_type' if x not in device_match_dict else x)\n",
    "test_identity['DeviceInfo_c'] = test_identity['DeviceInfo_c'].apply(lambda x: 'other_d_type' if x not in device_match_dict else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Device info 2\n",
    "train_identity['id_30'] = train_identity['id_30'].fillna('unknown_device').str.lower()\n",
    "test_identity['id_30'] = test_identity['id_30'].fillna('unknown_device').str.lower()\n",
    "\n",
    "train_identity['id_30_c'] = train_identity['id_30']\n",
    "test_identity['id_30_c'] = test_identity['id_30']\n",
    "\n",
    "device_match_dict = {\n",
    "    'ios':'ios',\n",
    "    'windows':'windows',\n",
    "    'mac':'mac',\n",
    "    'android':'android'\n",
    "}\n",
    "for dev_type_s, dev_type_o in device_match_dict.items():\n",
    "    train_identity['id_30_c'] = train_identity['id_30_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    test_identity['id_30_c'] = test_identity['id_30_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    \n",
    "train_identity['id_30_v'] = train_identity['id_30'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "test_identity['id_30_v'] = test_identity['id_30'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "        \n",
    "train_identity['id_30_v'] = np.where(train_identity['id_30_v']!='', train_identity['id_30_v'], 0).astype(int)\n",
    "test_identity['id_30_v'] = np.where(test_identity['id_30_v']!='', test_identity['id_30_v'], 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Browser\n",
    "train_identity['id_31'] = train_identity['id_31'].fillna('unknown_br').str.lower()\n",
    "test_identity['id_31']  = test_identity['id_31'].fillna('unknown_br').str.lower()\n",
    "\n",
    "train_identity['id_31'] = train_identity['id_31'].apply(lambda x: x.replace('webview','webvw'))\n",
    "test_identity['id_31']  = test_identity['id_31'].apply(lambda x: x.replace('webview','webvw'))\n",
    "\n",
    "train_identity['id_31'] = train_identity['id_31'].apply(lambda x: x.replace('for',' '))\n",
    "test_identity['id_31']  = test_identity['id_31'].apply(lambda x: x.replace('for',' '))\n",
    "\n",
    "browser_list = set(list(train_identity['id_31'].unique()) + list(test_identity['id_31'].unique()))\n",
    "browser_list2 = []\n",
    "for item in browser_list:\n",
    "    browser_list2 += item.split(' ')\n",
    "browser_list2 = list(set(browser_list2))\n",
    "\n",
    "browser_list3 = []\n",
    "for item in browser_list2:\n",
    "    browser_list3 += item.split('/')\n",
    "browser_list3 = list(set(browser_list3))\n",
    "\n",
    "for item in browser_list3:\n",
    "    train_identity['id_31_e_'+item] = np.where(train_identity['id_31'].str.contains(item),1,0).astype(np.int8)\n",
    "    test_identity['id_31_e_'+item] = np.where(test_identity['id_31'].str.contains(item),1,0).astype(np.int8)\n",
    "    if train_identity['id_31_e_'+item].sum()<100:\n",
    "        del train_identity['id_31_e_'+item], test_identity['id_31_e_'+item]\n",
    "        \n",
    "train_identity['id_31_v'] = train_identity['id_31'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "test_identity['id_31_v'] = test_identity['id_31'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "        \n",
    "train_identity['id_31_v'] = np.where(train_identity['id_31_v']!='', train_identity['id_31_v'], 0).astype(int)\n",
    "test_identity['id_31_v'] = np.where(test_identity['id_31_v']!='', test_identity['id_31_v'], 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Merge Identity columns\n",
    "temp_df = train_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(train_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "train_df = pd.concat([train_df,temp_df], axis=1)\n",
    "    \n",
    "temp_df = test_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(test_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "test_df = pd.concat([test_df,temp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Freq encoding\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10',\n",
    "          'id_11','id_13','id_14','id_17','id_18','id_19','id_20','id_21','id_22','id_24',\n",
    "          'id_25','id_26','id_30','id_31','id_32','id_33','id_33_0','id_33_1',\n",
    "          'DeviceInfo','DeviceInfo_c','id_30_c','id_30_v','id_31_v',\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### ProductCD and M4 Target mean\n",
    "for col in ['ProductCD','M4']:\n",
    "    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n",
    "                                                        columns={'mean': col+'_target_mean'})\n",
    "    temp_dict.index = temp_dict[col].values\n",
    "    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n",
    "    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Encode Str columns\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "        \n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TransactionAmt\n",
    "\n",
    "# Let's add some kind of client uID based on cardID ad addr columns\n",
    "# The value will be very specific for each client so we need to remove it\n",
    "# from final feature. But we can use it for aggregations.\n",
    "train_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card4'].astype(str)\n",
    "test_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card4'].astype(str)\n",
    "\n",
    "train_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "# Check if Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with model we are telling to trust or not to these values  \n",
    "valid_card = train_df['TransactionAmt'].value_counts()\n",
    "valid_card = valid_card[valid_card>10]\n",
    "valid_card = list(valid_card.index)\n",
    "    \n",
    "train_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\n",
    "test_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n",
    "\n",
    "# For our model current TransactionAmt is a noise (even when features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "i_cols = ['card1','card2','card3','card5','uid','uid2']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        new_col_name = col+'_TransactionAmt_'+agg_type\n",
    "        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "        train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "        test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "\n",
    "# Small \"hack\" to transform distribution \n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score \n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "train_df['TransactionAmt'] = np.log1p(train_df['TransactionAmt'])\n",
    "test_df['TransactionAmt'] = np.log1p(test_df['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Anomaly Search in geo information\n",
    "\n",
    "# Let's look on bank addres and client addres matching\n",
    "# card3/card5 bank country and name?\n",
    "# Addr2 -> Clients geo position (country)\n",
    "# Most common entries -> normal transactions\n",
    "# Less common etries -> some anonaly\n",
    "train_df['bank_type'] = train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\n",
    "test_df['bank_type']  = test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n",
    "\n",
    "train_df['address_match'] = train_df['bank_type'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['address_match']  = test_df['bank_type'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "for col in ['address_match','bank_type']:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    temp_df[col] = np.where(temp_df[col].str.contains('nan'), np.nan, temp_df[col])\n",
    "    temp_df = temp_df.dropna()\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(fq_encode)\n",
    "    test_df[col]  = test_df[col].map(fq_encode)\n",
    "\n",
    "train_df['address_match'] = train_df['address_match']/train_df['bank_type'] \n",
    "test_df['address_match']  = test_df['address_match']/test_df['bank_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Features elimination \n",
    "from scipy.stats import ks_2samp\n",
    "features_check = []\n",
    "columns_to_check = set(list(train_df)).difference(base_columns)\n",
    "for i in columns_to_check:\n",
    "    features_check.append(ks_2samp(test_df[i], train_df[i])[1])\n",
    "\n",
    "features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "features_discard = list(features_check[features_check==0].index)\n",
    "print(features_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model Features \n",
    "## We can use set().difference() but order matters\n",
    "## Matters only for deterministic results\n",
    "## In case of remove() we will not change order\n",
    "## even when variable will be renamed\n",
    "## please see this link to see how set is ordered\n",
    "## https://stackoverflow.com/questions/12165200/order-of-unordered-python-sets\n",
    "rm_cols = [\n",
    "    'TransactionID','TransactionDT', # These columns are pure noise right now\n",
    "    TARGET,                          # Not target in features))\n",
    "    'uid','uid2',                    # Our new clien uID -> very noisy data\n",
    "    'bank_type',                     # Victims bank could differ by time\n",
    "]\n",
    "features_columns = list(train_df)\n",
    "for col in rm_cols + features_discard:\n",
    "    if col in features_columns:\n",
    "        features_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':1,\n",
    "                    'n_estimators':800,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            vl_data = lgb.Dataset(P, label=P_y) \n",
    "        else:\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "            print(feature_imp)\n",
    "        \n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    \n",
    "    return tt_df\n",
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model Train\n",
    "if LOCAL_TEST:\n",
    "    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "else:\n",
    "    lgb_params['learning_rate'] = 0.005\n",
    "    lgb_params['n_estimators'] = 1800\n",
    "    lgb_params['early_stopping_rounds'] = 100    \n",
    "    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, NFOLDS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "if not LOCAL_TEST:\n",
    "    test_predictions['isFraud'] = test_predictions['prediction']\n",
    "    test_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
